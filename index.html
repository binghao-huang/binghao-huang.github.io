
<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Yuzhe Qin</title>
  <!-- Bulma Version-->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
  <script src="js/menuspy.js"></script>

  <!--  css-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
        integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="https://pro.fontawesome.com/releases/v5.10.0/css/all.css"
        integrity="sha384-AYmEC3Yw5cVb3ZcuHtOA93w35dYTsvhLPVnYs9eStHfGJvOvKxVfELGroGkvsg+p" crossorigin="anonymous"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="css/style.css" rel="stylesheet" type="text/css">
  <link href="css/css.css" rel="stylesheet">
  <link href="css/svg.css" rel="stylesheet" type="text/css">
</head>

<body>
<section class="section">
  <div class="container">
    <div class="columns">
      <div class="column is-2">
        <div class="sticky">
          <figure class="image is-128x128">
<!--             <img class="is-rounded" src="file/qyz_circle.png"> -->
          </figure>
          <div class="content">
            <h3>Yuzhe Qin</h3>
            <h6>Robot Learning</h6>
            <h6>UC San Diego</h6>
          </div>
          <div class="details">
            <h3>EMAIL</h3>
            <p><a href="mailto:y1qin@ucsd.edu">y1qin<br>[at]ucsd[dot]edu</a></p>
          </div>
          <div class="social">
            <a href="https://github.com/yzqin" target="_blank">
              <i class="fab fa-github-square svg-inline--fa fa-w-16 fa-2x"></i>
            </a>
            <a href="https://scholar.google.com/citations?user=3KF3AIMAAAAJ&hl=en" target="_blank">
              <i class="ai ai-google-scholar-square svg-inline--fa fa-w-16 fa-2x"></i>
            </a>
            <a href="https://twitter.com/QinYuzhe" target="_blank">
              <i class="fab fa-twitter-square svg-inline--fa fa-w-16 fa-2x"></i>
            </a>
          </div>
          <div id="sidebar" class="menu sticky is-hidden-mobile">
            <p class="menu-label"><b>Quick Links</b></p>
            <ul class="menu-list">
              <li><a href="#bio">Bio</a></li>
              <li><a href="#research" class="is-active">Publication</a></li>
              <li><a href="#work">Work Experience</a></li>
              <li><a href="#professional">Professional Activities</a></li>
            </ul>
          </div>
        </div>
      </div>
      <div class="column">
        <div class="content">
          <h3 id="bio">
            Bio
          </h3>
          <p>
            I am a second-year Ph.D. student at UC San Diego,
            coadvised by Prof. <a href="https://xiaolonw.github.io/" target="_blank"
                                  style="text-decoration: underline;">Xiaolong Wang</a>
            and Prof. <a href="https://cseweb.ucsd.edu/~haosu/" target="_blank"
                         style="text-decoration: underline;">Hao Su</a>. Prior to joining UCSD, I received
            my bachelor degree from Shanghai Jiao Tong University.

            <br/><br/>
            My research interests lie in building robot that can learn from interacting with the world. In
            particular, I am interested in robotics manipulation, task-oriented 3D perception,
            imitation learning from human demonstrations, and building robotic simulation environment.

          </p>

          <!--Research-->
          <h3 id="research">Publications</h3>
          <br/>
<!--           <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/dexteleop.gif" alt="file missing" style="margin-top: 10%">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>From One Hand to Multiple Hands: Imitation Learning for Dexterous Manipulation from Single-Camera
                    Teleoperation</b><br>
                  <strong>Yuzhe Qin</strong>, Hao Su*, Xiaolong Wang*<br>
                  <em>IEEE Robotics and Automation Letters <strong>(RA-L)</strong> 2022</em><br>
                  <em>International Conference on Intelligent Robots and Systems <strong>(IROS)</strong> 2022</em><br>
                  <a href="https://yzqin.github.io/dex-teleop-imitation/" target="_blank">[<u>project</u>]</a>
                  <a href="https://arxiv.org/abs/2204.12490" target="_blank">[<u>paper</u>]</a>
                  <a href="https://www.youtube.com/watch?v=JoRypQJB6mc" target="_blank">[<u>video</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/dexmv.gif" alt="file missing" style="margin-top: 10%">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>DexMV: Imitation Learning for Dexterous Manipulation from Human Videos</b><br>
                  <strong>Yuzhe Qin*</strong>, Yueh-Hua Wu*, Shaowei Liu*, Hanwen Jiang*, Ruihan Yang,
                  Yang Fu, Xiaolong Wang<br>
                  <em>European Conference on Computer Vision <strong>(ECCV)</strong> 2022</em><br>
                  <a href="https://yzqin.github.io/dexmv/" target="_blank">[<u>project</u>]</a>
                  <a href="https://arxiv.org/abs/2108.05877" target="_blank">[<u>paper</u>]</a>
                  <a href="https://www.youtube.com/watch?v=BPLIHMIeivE" target="_blank">[<u>video1</u>]</a>
                  <a href="https://www.youtube.com/watch?v=XV4MpY5PPj0" target="_blank">[<u>video2</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/kuafu_sim_to_real.jpeg" alt="file missing" style="margin-top: 10%">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>Close the Visual Domain Gap by Physics-Grounded Active Stereovision Depth Sensor Simulation</b><br>
                  Xiaoshuai Zhang*, Rui Chen*, Fanbo Xiang, <strong>Yuzhe Qin</strong>, Jiayuan Gu, Zhan Ling, Minghua
                  Liu,
                  Peiyu Zeng, Songfang Han, Zhiao Huang, Tongzhou Mu, Jing Xu, Hao Su<br>
                  <em>ArXiv 2022</em><br>
                  <a href="https://arxiv.org/abs/2201.11924" target="_blank">[<u>paper</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/mmdr.gif" alt="file missing" style="margin-top: 0%">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization</b><br>
                  Chieko Sarah Imai*, Minghao Zhang*, Yuchen Zhang*, Marcin Kierebi≈Ñski, <br/>
                  Ruihan Yang, <strong>Yuzhe Qin</strong>, Xiaolong Wang<br>
                  <em>International Conference on Intelligent Robots and Systems <strong>(IROS)</strong> 2022</em><br>
                  <a href="https://mehooz.github.io/mmdr-wild/" target="_blank">[<u>project</u>]</a>
                  <a href="https://arxiv.org/abs/2109.14549" target="_blank">[<u>paper</u>]</a>
                  <a href="https://github.com/Mehooz/vision4leg" target="_blank">[<u>code</u>]</a>
                  <a href="https://www.youtube.com/watch?v=k9VsI5AkJFw&feature=youtu.be" target="_blank">[<u>video</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/o2oafford.png" alt="file missing">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning</b><br>
                  Kaichun Mo, <strong>Yuzhe Qin</strong>, Fanbo Xiang, Hao Su and Leonidas J.
                  Guibas<br>
                  <em>Conference of Robot Learning <strong>(CoRL)</strong> 2021</em><br>
                  <a href="https://cs.stanford.edu/~kaichun/o2oafford/"
                     target="_blank">[<u>project</u>]</a>
                  <a href="https://arxiv.org/abs/2106.15087" target="_blank">[<u>paper</u>]</a>
                  <a href="https://github.com/daerduoCarey/o2oafford" target="_blank">[<u>code</u>]</a>
                  <a href="https://www.youtube.com/watch?v=cbDSalrMhlo" target="_blank">[<u>video</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/ocrtoc.gif" alt="file missing">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>OCRTOC: A Cloud-Based Competition and Benchmark for Robotic
                    Grasping and Manipulation</b><br>
                  Ziyuan Liu, Wei Liu, <strong>Yuzhe Qin</strong>, Fanbo Xiang, Songyan Xin, Maximo A
                  Roa, <br/>Berk Calli, Hao Su, Yu Sun, Ping Tan<br>
                  <em>IEEE Robotics and Automation Letters <strong>(RA-L)</strong> 2021</em><br>
                  <a href="http://www.ocrtoc.org/" target="_blank">[<u>website</u>]</a>
                  <a href="https://arxiv.org/pdf/2104.11446.pdf" target="_blank">[<u>paper</u>]</a>
                  <a href="https://github.com/OCRTOC/OCRTOC_software_package"
                     target="_blank">[<u>code</u>]</a>
                  <a href="https://www.youtube.com/watch?v=9dlWUbPF2Cc" target="_blank">[<u>video</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/captra.gif" alt="file missing">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point
                    Clouds</b><br>
                  Yijia Weng*, He Wang*, Qiang Zhou, <strong>Yuzhe Qin</strong>, Yueqi Duan, Qingnan
                  Fan, <br/>Baoquan Chen, Hao Su, Leonidas J. Guibas<br>
                  <em>International Conference on Computer Vision <strong>(ICCV)</strong> 2021 <em
                      style="color:red;">(Oral)</em></em><br>
                  <a href="https://yijiaweng.github.io/CAPTRA/" target="_blank">[<u>project</u>]</a>
                  <a href="https://arxiv.org/abs/2104.03437" target="_blank">[<u>paper</u>]</a>
                  <a href="https://github.com/halfsummer11/CAPTRA"
                     target="_blank">[<u>code</u>]</a>
                  <a href="https://www.youtube.com/watch?v=JFPcOHCH2O0&feature=youtu.be"
                     target="_blank">[<u>video</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/sapien.gif" alt="file missing">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>SAPIEN: A SimulAted Part-based Interactive ENvironment</b><br>
                  Fanbo Xiang,
                  <strong>Yuzhe Qin</strong>,
                  Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, <br/>Hanxiao Jiang,
                  Yifu Yuan, He Wang, Li Yi, Angel X.Chang, Leonidas J. Guibas and Hao Su <br/>
                  <em>Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong> 2020 <em
                      style="color:red;">(Oral)</em></em><br>
                  <a href="https://sapien.ucsd.edu/" target="_blank">[<u>project</u>]</a>
                  <a href="https://arxiv.org/pdf/2003.08515.pdf" target="_blank">[<u>paper</u>]</a>
                  <a href="https://github.com/haosulab/SAPIEN-Release"
                     target="_blank">[<u>code</u>]</a>
                  <a href="https://www.youtube.com/watch?v=UKE75u51XN4" target="_blank">[<u>video</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/s4g.gif" alt="file missing">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in
                    Cluttered Scenes</b><br>
                  <strong>Yuzhe Qin*</strong>,
                  Rui Chen*, Hao Zhu, Meng Song, Jing Xu, Hao Su<br>
                  <em>Conference of Robot Learning <strong>(CoRL)</strong> 2019</em><br>
                  <a href="https://sites.google.com/view/s4ggrapsing" target="_blank">[<u>project</u>]</a>
                  <a href="https://arxiv.org/pdf/1910.14218.pdf" target="_blank">[<u>paper</u>]</a>
                  <a href="https://github.com/yzqin/s4g-release" target="_blank">[<u>code</u>]</a>
                  <a href="https://www.youtube.com/watch?v=Xlq4nw2AGcY" target="_blank">[<u>video</u>]</a>
                </p>
              </div>
            </div>
          </article>

          <article class="columns">
            <div class="column is-3">
              <figure class="image">
                <img src="img/composing_rl.gif" alt="file missing">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b>Composing Task-Agnostic Policies with Deep Reinforcement Learning</b><br>
                  Ahmed Qureshi, Jacob Johnson, <strong>Yuzhe Qin</strong>,
                  Taylor Henderson, Byron Boots, Michael Yip<br>
                  <em>International Conference of Learning Representation <strong>(ICLR)</strong> 2020</em><br>
                  <a href="https://sites.google.com/view/compositional-rl"
                     target="_blank">[<u>project</u>]</a>
                  <a href="https://arxiv.org/pdf/1905.10681.pdf" target="_blank">[<u>paper</u>]</a>
                  <a href="https://github.com/ahq1993/compositional_reinforcement_learning"
                     target="_blank">[<u>code</u>]</a>
                </p>
              </div>
            </div>
          </article> -->

          <!--Work-->
          <h3 id="work">Work Experience</h3>
          <br>
<!--           <article class="columns">
            <div class="column is-2">
              <figure class="image">
                <img src="img/googleX.png" alt="file missing">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b><a href="https://everydayrobots.com/">AI Resident, Google X, Everyday Robot Project</a></b><br>
                  05/15/2020 - 09/24/2020 <br/><br/>
                  Mentor: <a href="https://itsdanielho.com/" target="_blank"
                             style="text-decoration: underline;">Daniel Ho</a> and <a
                    href="https://www.yunfei-bai.com/" target="_blank"
                    style="text-decoration: underline;">Yunfei Bai</a>
                </p>
              </div>
            </div>
          </article> -->
<!-- 
          <article class="columns">
            <div class="column is-2">
              <figure class="image">
                <img src="img/nvidia.png" alt="file missing">
              </figure>
            </div>
            <div class="column">
              <div class="content">
                <p>
                  <b><a href="https://www.nvidia.com/en-us/research/robotics/">Research Intern, NVIDIA, Seattle Robotics
                    Lab</a></b><br>
                  06/12/2022 - Now <br/><br/>
                  Mentor:
                  <a href="http://wyang.me" target="_blank"
                     style="text-decoration: underline;">Wei Yang</a>,
                  <a href="https://research.nvidia.com/person/yu-wei-chao" target="_blank"
                     style="text-decoration: underline;">Yu-Wei Chao</a>,
                  and <a href="https://homes.cs.washington.edu/~fox/" target="_blank"
                         style="text-decoration: underline;">Dieter Fox</a>
                </p>
              </div>
            </div>
          </article> -->

          <h3 id="professional">Professional Activities</h3>
          <ul>
            <li>Tutorial Organizer: <a href="https://ai-workshops.github.io/building-and-working-in-environments-for-embodied-ai-cvpr-2022/" target="_blank">
              Building and Working in Environments for Embodied AI</a> @CVPR 2022;
            </li>
            <li>Challenge Organizer: <a href="http://www.ocrtoc.org" target="_blank">Open Cloud Robot Table
              Organization Challenge (OCRTOC)</a> @IROS 2020, @ICRA 2021;
            </li>
            <li>Conference Reviewer: AAAI 2020; CVPR 2021, 2022; ICRA 2021, 2022; ICCV 2021; IROS 2021, 2022; <br>
              ECCV 2022; CoRL 2022; SIGGRAPH 2022</li>
            <li>Journal Reviewer: Robotics and Automation Letters (RA-L);</li>
            <li>Workshop Reviewer: Simulation Technology for Embodied AI (SEAI2021);</li>
          </ul>
        </div>
        <br>
        <p style="text-align:left;font-size:small;">
          Credit: web source from <a href="http://hansf.me">Dr. Songfang Han</a>
        </p>
      </div>
    </div>
  </div>
</section>
<script>
    var elm = document.querySelector('#sidebar');
    var ms = new MenuSpy(elm, {
        activeClass: 'is-active'
    });
</script>
</body>
</html>
